# Úvod do strojového učení v Pythonu

- https://ufal.mff.cuni.cz/courses/npfl129
- https://github.com/ufal/npfl129/
- Piazza
	- primární způsob komunikace
	- čte ji víc lidí než e-mail, takže je větší šance, že nám někdo odpoví
	- nedávat tam zdrojový kód (do veřejných otázek)
- bodování
	- zápočet – alespoň 70 standardních bodů
	- standardní body nad 70 se přenáší ke zkoušce
	- bonusové body lze získat soutěžemi a dalšími aktivitami nad rámec, také se přenáší ke zkoušce
- definice strojového učení
	- pomocí zkušenosti (dat) se zlepšuje metrika na nějaké úloze
	- možné úlohy: klasifikace (přiřazení vstupu do diskrétní kategorie), regrese (přiřazení reálného čísla nebo vektoru reálných čísel), předvídání struktury, odstraňování šumu
	- metriky: accuracy (jak dobře se při klasifikaci trefujeme), …
	- zkušenost
		- supervised (učení s učitelem) – máme olablovaná data
		- unsupervised (učení bez učitele) – nemáme žádné labely, učíme se vidět vzory v datech
		- reinforcement learning (zpětnovazebné učení) – např. aby byl šachový program lepší než nejlepší hráči
- základní úlohy
	- mějme vstup $x\in\mathbb R^D$
	- regrese: pro $x$ hledáme reálnou proměnnou $t\in\mathbb R$
	- klasifikace: máme $K$ labelů, pro dané $x$ chceme najít ten správný
		- můžeme určit třídu
		- můžeme určit pravděpodobnostní distribuci tříd
	- obvykle máme trénovací sadu dvojic $(x,t)$
	- rozdělíme data na trénovací a testovací – aby bylo učení dost obecné
- notace
	- tenzor je nadkrychle čísel
	- všechny vektory jsou sloupcové
		- do matic je skládáme jako řádky
- vstupní data
	- trénovací množina $X\in\mathbb R^{N\times D}$
		- $N$ instancí, každé odpovídá $D$ reálných čísel
		- dimenze vstupu = feature
	- při učení s učitelem máme target $t$ pro každou instanci
		- v regresi … $t\in\mathbb R^N$
		- v klasifikaci … $t\in\set{0,1,\dots,K-1}$

## Lineární regrese

- lineární regrese
	- $y(x;w,b)=x^Tw+b$
		- $w$ … váhy
		- $b$ … bias (jindy se označuje jako intercept)
		- protože je otrava zacházet s biasem, někdy se k vektorům přidávají na konec jedničky, takže místo $b$ máme $1\cdot w_{d+1}$
	- chceme změřit, jak se nám regrese daří
		- použijeme mean squared error (MSE) nebo taky funguje součet druhých mocnin chyb (dělený dvěma)
	- co když matice $X^TX$ není regulární? můžeme přičíst náhodný šum (dost malý)
- overfitting, underfitting
	- overfitting … model funguje dobře na trénovacích datech, ale nefunguje na testovacích datech
	- kapacita modelu – slouží k ovlivňování underfittingu a overfittingu
		- representational capacity
		- effective capacity
		- čím větší kapacita modelu, tím větší pravděpodobnost overfittingu
	- dvě křivky – chyba na trénovacích a testovacích datech
		- mezera mezi nimi = generalization gap
	- cílem je najít minimum křivky chyby na trénovacích datech
		- "sweet spot"
	- jak se zbavit overfittingu
		- použít více dat
		- regularizace – jakýkoliv krok k větší generalizaci modelu
			- příkladem regularizace je snižování kapacity modelu
	- $L^2$-regularizace
		- upřednostňuje „jednodušší“ modely, tedy ty, které mají menší váhy
		- obvykle se aplikuje na opravdové váhy, ne na bias
		- důsledkem je omezení efektivní kapacity modelu
			- pro větší lambda nám kapacita přestane stačit – daty se v podstatě proloží přímka
			- lambda … regularization rate 
		- matice, kterou nakonec použijeme, je nutně regulární, takže jsme vyřešili problém s inverzní maticí
- volba hyperparametrů
	- učící algoritmus nenastavuje hyperparametry
	- validační/vývojová sada dat se používá k určení hyperparametrů
	- takže data dělíme mezi trénovací, vývojová a testovací (obvykle v poměru 60 %, 20 %, 20 %)
	- zatím jsme měli hyperparametry $M$ (stupeň polynomu) a $\lambda$ („úroveň regularizace“)
- gradient descent
	- někdy se hodí hledat nejlepší váhy iterativně
	- hledáme $\text{arg}_w\min E(w)$
	- můžeme použít gradient descent $w\leftarrow w-\alpha\nabla_wE(w)$
	- $\alpha$ … learning rate
	- přístupy
		- standard/batch gradient descent – použijeme všechna trénovací data
		- stochastic/online gradient descent (SGD) – po jednom vybíráme náhodné příklady z trénovacích dat
			- unbiased, but noisy
		- minibatch stochastic gradient descent
	- dá se dokázat, že SGD za určitých podmínek téměř jistě konvrguje
- feature types
	- polynomial features
	- categorical one-hot features
- features je potřeba normalizovat, abychom mohli pro všechny používat stejný learning rate
	- odečtením minima a vydělením rozdílem maxima a minima data dostaneme mezi nulu a jedničku
	- velký problém jsou outliers
		- dá se to řešit standardizací
		- data budou mezi -1 a 1
		- nebo se můžeme zbavit outlierů
