# Data Compression Algorithms

## Introduction

- data compression
	- process of converting an input data stream into output data stream that has a smaller size
	- compression algorithm = encoding (compression) + decoding (decompression)
- compression
	- lossless – the restored and original data are identical
	- lossy – the restored data are a reasonable approximation of the original
- methods
	- static / adapative
	- streaming / block
		- block – we compress the data by blocks
- goals – to save storage, to reduce the transmission bandwidth
- measuring the performance
	- units
		- input data size … $u$ bytes (uncompressed)
		- compressed data size … $k$ bytes, $K$ bits
		- compression ratio … $k/u$ (written as percentage)
		- compression factor … $u:k$
		- compression gain … $(u-k)/u$ (written as percentage)
		- average codeword length … $K/u$ (may be bpc or bpp)
			- bpc … bits per char
			- bpp … bits per pixel
		- relative compression (percent log ratio) … $100\ln(k/k')$
			- $k'$ … size of data compressed by a standard algorithm
	- data corpora
		- Calgary Corpus – 14 files: text, graphics, binary files
		- Canterbury Corpus – 11 files + artificial corpus (4) + large corpus (3) + miscellaneous corpus (1)
		- Silesia Corpus
		- Prague Corpus
	- contests
		- Calgary Corpus Compression Challenge
		- Hutter Prize – Wikipedia compression
			- goal – encourage research in AI
- limits of lossless compression
	- encoding $f:$ {$n$-bit strings} → {strings of length $\lt n$}
		- $|\text{Dom}f|=2^n$
		- $|\text{Im}f|\leq 2^n-1$
		- such $f$ cannot be injective → does not have an inverse function
	- let $M\subseteq\text{Dom} f$ such that $\forall s\in M:|f(s)|\leq 0.9n$
		- $f$ injective on $M\implies|M|\leq2^{1+0.9n}-1$
		- $n=100$ … $|M|/2^n\lt 2^{-9}$
		- $n=1000$ … $|M|/2^n\lt 2^{-99}\approx 1.578\cdot 10^{-30}$
	- we can't compress all data efficiently (but that does not matter, we often focus on specific instances of data)

## Statistical methods

- basic concepts
	- source alphabet … $A$
	- coding alphabet … $A_C$
	- encoding … $f:A^*\to A^*_C$
	- $f$ injective → uniquely decodable encoding
- typical strategy
	- input string is factorized into concatenation of substrings
		- $s=s_1s_2\dots s_k$
	- substrings = phrases
	- $f(s)=C(s_1)C(s_2)\dots C(s_k)$
		- $C(s_i)$ … codeword
- codes
	- source code … $K:A\to A^*_c$
	- $K(s)$ … codeword for symbol $s\in A$
	- encoding $K^*$ generated by code $K$ is the mapping $K^*(s_1s_2\dots s_n)=K(s_1)K(s_2)\dots K(s_n)$ for every $s\in A^*$
	- $K$ is uniquely decodable $\equiv$ generates a uniquely decodable encoding $K^*$
	- example
		- $\set{0,01,11}$ … is uniquely decodable
		- $\set{0,01,10}$ … is not, counterexample: $010$
		- $K_1$ … fixed length
		- $K_2$ … no codeword is a prefix of another one
- prefix codes
	- prefix code is a code such that no codeword is a prefix of another codeword
	- observation
		- every prefix code $K$ (over a binary alphabet) may be represented by a binary tree = prefix tree for $K$
		- prefix tree may be used for decoding
	- idea: some letters are more frequent, let's assign shorter codewords to them
- Shannon-Fano coding
	- input – source alphabet $A$, frequency $f(s)$ for every symbol $s\in A$
	- output – prefix code for $A$
	- algorithm
		- sort the source alphabet by symbol frequencies
		- divide the table into parts with similar sums of frequencies
		- append 0/1 to prefix, repeat
	- definition: the algorithm constructs an optimal prefix / uniquely decodable code $K\equiv$ $|K'^*(a_1a_2\dots a_n)|\geq |K^*(a_1a_2\dots a_n)|$ for every prefix (uniquely decodable) code $K'$ and for every string $a\in A^*$ with symbol frequencies $f$
	- Shannon-Fano is not optimal
		- Fano's student David Huffman described a construction of optimal prefix code
	- divide and conquer algorithm
- Huffman code
	- constructs the tree from the bottom
	- starts with the symbols that have the lowest frequencies
	- sum of the frequencies is used as the frequency of the new node
	- greedy algorithm
	- interesting type of Huffman code: [Canonical Huffman code](https://en.wikipedia.org/wiki/Canonical_Huffman_code)
	