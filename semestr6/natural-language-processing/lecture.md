# Lecture

- credit
	- homeworks
	- attendance to practicals is mandatory, 3 absences allowed at maximum
	- final test

## Probability, information theory

- sample space $\Omega$
- event $A$ as a set of basic outcomes
	- $A\subseteq\Omega$
- we can estimate the probability of event $A$ by experiment
	- we divide the number of $A$ occurring by the number of experiments
	- maximum likelihood estimation
- axioms
	- $p(A)\in[0,1]$
	- $p(\Omega)=1$
	- $p(\bigcup A_i)=\sum p(A_i)$
- joint probability, conditional probability
- estimating conditional probability
- Bayes rule
- independence
- chain rule
- golden rule of statistical NLP
- expectation
- entropy
	- nothing can be more uncertain than the uniform distribution
- perplexity
	- $G(p)=2^{H(p)}$
- joint entropy, conditional entropy
- entropy is non-negative
- chain rule
	- $H(X,Y)=H(Y\mid X)+H(X)$
- $H(Y\mid X)\leq H(Y)$
- other properties of entropy
- coding interpretation
	- entropy … the least average number of bits needed to encode a message
- KL distance (divergence)
- mutual information
	- $I(X,Y)=D(p(x,y)\Vert p(x)p(y))$
	- we can derive that $I(X,Y)=H(X)-H(X\mid Y)$
		- by symmetry $I(X,Y)=H(Y)-H(Y\mid X)$
- cross-entropy

## Noisy channel model

- we try to recover the original input from a noised output
	- original input $A$ = the message in someone's mind
	- noised output $B$ = the written/spoken representation
- usage: OCR, handwriting, speech recognition, machine translation, POS tagging
- to get input
	- $p(A\mid B)=p(B\mid A)\cdot\frac{p(A)}{p(B)}$
	- $A^*=\text{argmax}_A\;p(B\mid A)\cdot p(A)$
		- $p(B\mid A)$ … acoustic model
		- $p(A)$ … language model
- language modelling
	- we need to model the probabilities of sequences of words
	- we will use n-gram probabilities so that we don't need many parameters
		- $p(w_i\mid w_{i-1},w_{i-2})=\frac{c(w_{i-2},w_{i-1},w_i)}{c(w_{i-2},w_{i-1})}$
	- number of parameters
		- uniform … 1
		- unigram … $|V|-1$
		- bigram … $\sim|V|^2$
		- trigram … $\sim|V|^3$
			- for $|V|\approx 60k$, it is larger than the number of parameters of the Llama model
- smoothing
	- we can add 1 to all of the counts
		- ADD-1 smoothing
		- $p(w\mid h)=\frac{c(w,h)+1}{c(h)+|V|}$
		- what if there is an word that we did not see in the training set? we will have a special word for that `<unk>`
	- we can add $\lambda$ to all of the counts
		- ADD-$\lambda$
		- $p(w\mid h)=\frac{c(w,h)+\lambda}{c(h)+\lambda\cdot |V|}$
		- how to estimate $\lambda$
		- we want to minimize the cross-entropy
		- we cannot use training (nor test) data for that, we need holdout (dev) data
		- $p(w_i\mid w_{i-2},w_{i-1})=\lambda_0\cdot p_0+\lambda_1 \cdot p_1(w_i)+\lambda_2\cdot p_2(w_i\mid w_{i-1})+\lambda_3\cdot p_3(w_i\mid w_{i-1},w_{i-2})$
			- $\sum\lambda_i=1$
		- expectation maximization (EM) algorithm
			- expectation: $c(\lambda_j)=\sum\lambda _jp_j(w\mid h)/p'(w\mid h)$
			- maximization: $\lambda_j=\frac{c(\lambda_j)}{\sum c(\lambda_h)}$
- homework
	- language identification using char-level language model
	- at least 2 languages

## Morphological analysis

- morphological annotation
	- POS tags
- tagsets
	- English: Penn Treebank (45 tags), Brown Corpus (87), Claws c5 (62), London-Lund (197)
	- Czech: Prague Dependency Treebank (4294; positional), Multext-East (1485; Orwell 1984 parallel corpus), Prague Spoken Corpus (over 10000)
	- Universal Dependencies: 17 universal POS tags, 27 universal features (each with 1–37 possible values)
- Czech positional tags of PDT
	- positions: part of speech, subpos, gender, number, case, poss gender, poss number, person, tense, degree, polarity, voice, (reserved), (reserved), style
	- gender ambiguities → more values of the position
- Penn Treebank tagset
	- prepositions and subordinate conjunctions have the same tag (hard to distinguish)
	- “to” has its own tag (it denotes and infinitive or works as a preposition, not easy to distinguish)
- universal POS tags (from Universal Dependencies)
	- noun, proper noun, verb, adjective, adverb, interjection, pronoun, determiner, auxiliary, numeral, adposition, subordinating conjunction, coordinating conjunction, particle, punctuation, symbol, unknown
- ancient Greek word classes
	- adjectives are missing
		- they are a relatively new invention from France
		- in French, adjectives behave quite differently than nouns
- traditional parts of speech
	- English: noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection
	- Czech: noun, adjective, pronoun, numeral, verb, adverb, preposition, conjunction, particle, interjection
- openness vs. closeness, content vs. function words
	- open classes (take new words)
		- verbs (non-auxiliary), nouns, adjectives, adjectival adverbs, interjections
		- word formation (derivation) across classes
	- closed classes (words can be enumerated)
		- pronouns/determiners, adpositions, conjunctions particles
		- pronominal adverbs
		- auxiliary and modal verbs/particles
		- numerals
		- typically thay are not base for derivation
	- even closed classes evolve but over longer period of time
- morphological analysis
	- input: word form (token)
	- output
		- set of analyses (possibly empty)
		- an analysis
			- lemma (base form of the lexeme)
			- tag (morphological, POS)
				- POS
				- features and their values
- morphological analysis vs. tagging
	- tagging … context-based disambiguation
	- most taggers employ ML methods
	- taggers may or may not work on top of morphological analysis
- finite-state morphology
	- finite-state automaton/machine
	- example: FSA checking correct spelling of Czech *dě, tě, ně*
- lexicon is implemented as a FSA (trie)
	- composed of multiple sublexicons (prefixes, stems, suffixes)
	- notes (glosses) at the end of every sublexicon
		- e.g. POS tags
	- lexicon is a DAG, not a tree
- problem with phonology: baby+s → babies (not babys)
	- two-level mophology solves that
		- upper (lexical) language
		- lower (surface) language
	- two-level rules
		- lexical: `baby+0s`
		- surface: `babi0es`
	- zero is treated as a normal symbol (but corresponds to an empty string)
- finite-state transducer (převodník)
	- transudcer is a special case of automaton
	- checking (finite-state automaton)
		- does the word belong to the language (lexicon)?
	- analysis (finite-state transducer)
		- surface string → lexical string
	- generation (finite-state transducer)
		- lexical string → surface string
- another way of rule notation: two-level grammar
	- `a:b <=> l:l _ r:r`
	- lexical `a` must be realized as surface `b` in this context and only in this context
		- context … between `l` and `r`
	- FST can be constructed from that
