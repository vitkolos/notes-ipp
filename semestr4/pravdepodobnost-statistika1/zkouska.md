# Zkouška

## Základy pravděpodobnosti

- Věta o základních vlastnostech pravděpodobnosti
	- definice
		- $\mathcal F\subseteq\mathcal P(\Omega)$ je prostor jevů, pokud…
			- $\emptyset,\Omega\in\mathcal F$
			- $A\in\mathcal F\implies A^c=\Omega\setminus A\in\mathcal F$
			- $A_1,A_2,\ldots\in\mathcal F\implies\bigcup A_i\in \mathcal F$
		- $P:\mathcal F\to[0,1]$ je pravděpodobnost, pokud…
			- $P(\Omega)=1$
			- $P(\bigcup A_i)=\sum P(A_i)$ pro $A_1,A_2,\ldots\in \mathcal F$ po dvou disjunktní
	- věta
		- $P(A)+P(A^c)=1$
			- jsou disjunktní $\land\; A\cup A^c=\Omega\implies$použijeme druhý bod definice
		- $A\subseteq B\implies P(B\setminus A)=P(B)-P(A)\implies P(A)\leq P(B)$
			- $B$ rozložíme na dvě disjunktní množiny: $B=A\cup(B\setminus A)$
		- $P(A\cup B)=P(A)+P(B)-P(A\cap B)$
			- $A\cup B$ rozložíme na tři disjunktní množiny (rozdíly a průnik)
		- $P(A_1\cup A_2\cup\dots)\leq\sum_i P(A_i)$ … subaditivita, Booleova nerovnost
			- uvažujeme sjednocení disjunktních $B_i=A_i\setminus\bigcup_{j\lt i}A_j$
			- $\forall i:B_i\subseteq A_i$, tedy $P(B_i)\leq P(A_i)$
			- zjevně $\bigcup B_i=\bigcup A_i$
- Podmíněná pravděpodobnost, její zřetězení
	- pokud $A,B\in\mathcal F$ a $P(B)\gt 0$, definujeme podmíněnou pravděpodobnost $A$ při $B$ jako $P(A\mid B)=\frac{P(A\cap B)}{P(B)}$
	- zjevně $P(A\cap B)=P(A)\cdot P(B\mid A)$
	- $P(A_1\cap A_2\cap \dots\cap A_n)=P(A_1)P(A_2\mid A_1)P(A_3\mid A_1\cap A_2)\dots P(A_n\mid A_1\cap\dots\cap A_{n-1})$
		- lze ukázat indukcí (nebo neformálně rozepsáním členů vpravo a vykrácením)
- Věta o úplné pravděpodobnosti
	- věta
		- mějme $B_1,B_2,\dots$ rozklad $\Omega$
		- $P(A)=\sum_iP(B_i)\cdot P(A\mid B_i)$
			- sčítance s $P(B_i)=0$ považujeme za nulové
	- důkaz
		- $B_i$ tvoří rozklad, takže $A$ můžeme napsat jako disjunktní sjednocení množin $A\cap B_i$
		- pak $P(A)=\sum_i P(A\cap B_i)$
- Bayesova věta
	- věta
		- mějme $B_1,B_2,\dots$ rozklad $\Omega$
		- $P(B_j\mid A)=\frac{P(B_j)\cdot P(A\mid B_j)}{\sum_i P(B_i)\cdot P(A\mid B_i)}$
	- důkaz
		- $P(A)\cdot P(B_j\mid A)=P(A\cap B_j)=P(B_j)\cdot P(A\mid B_j)$
		- $P(A)=\sum_iP(B_i)\cdot P(A\mid B_i)$
- Nezávislost jevů
	- jevy $A,B\in\mathcal F$ jsou nezávislé, pokud $P(A\cap B)=P(A)\cdot P(B)$
	- můžeme uvažovat i větší množiny jevů
		- jevy v množině jsou (vzájemně) nezávislé, pokud pro každou konečnou podmnožinu platí, že $P(\bigcap)=\prod P$
		- pokud podmínka platí jen pro dvouprvkové podmnožiny, jsou jevy *po dvou nezávislé*

## Diskrétní náhodné veličiny

- Diskrétní náhodné veličiny: popis pomocí pravděpodobnostní funkce
	- $p_X(x)=P(\set{X=x})$
- Příklady diskrétních rozdělení: Bernoulliho, geometrické, binomické, Poissonovo
	- Bernoulliho/alternativní rozdělení $\text{Ber}(p)$
		- počet úspěchů při jednom pokusu (kde $p$ je pravděpodobnost úspěchu)
		- $p_X(1)=p$
		- $p_X(0)=1-p$
		- jinak $p_X(x)=0$
		- $\mathbb E(X)=p$
		- $\text{var}(X)=p(1-p)$
	- Geometrické rozdělení $\text{Geom}(p)$
		- při kolikátém pokusu poprvé uspějeme
		- $p_X(k)=(1-p)^{k-1}\cdot p$
		- $\mathbb E(X)=1/p$
		- $\text{var}(X)=\frac{1-p}{p^2}$
	- Binomické rozdělení $\text{Bin}(n,p)$
		- počet úspěchů při $n$ nezávislých pokusech
		- $p_X(k)={n\choose k}p^k(1-p)^{n-k}$
		- $\mathbb E(X)=np$
		- $\text{var}(X)=np(1-p)$
	- Poissonovo rozdělení $\text{Pois}(\lambda)$
		- počet doručených zpráv za časový úsek, $\lambda$ je průměrná hodnota
		- $p_X(k)=\frac{\lambda^k}{k!}e^{-\lambda}$
		- $\text{Pois}(\lambda)$ je limitou $\text{Bin}(n,\lambda/n)$ pro $n\to\infty$
			- $p_{X_n}(k)={n\choose k}(\frac\lambda n)^k(1-\frac\lambda n)^{n-k}$
			- $=\frac{n(n-1)\dots(n-k+1)}{k!}\frac{\lambda^k}{n^k}(1-\frac\lambda n)^n(1-\frac\lambda n)^{-k}$
			- $=\frac{\lambda^k}{k!}\underbrace{\frac{n(n-1)\dots(n-k+1)}{n^k}}_{\to\,1}\underbrace{(1-\frac\lambda n)^n}_{\to\,e^{-\lambda}}\underbrace{(1-\frac\lambda n)^{-k}}_{\to\,1}$
		- $\mathbb E(X)=\lambda$
		- $\text{var}(X)=\lambda$
- Střední hodnota diskrétní náhodné veličiny: definice, vlastnosti (linearita, podmíněná střední hodnota, věta o celkové střední hodnotě), výpočet
	- $\mathbb E(X)=\sum_{x\in\text{Im}(X)}x\cdot P(X=x)$
	- pozorování: $\mathbb E(X)=\sum_{\omega\in\Omega}X(\omega)\cdot P(\set{\omega})$
	- pravidlo naivního statistika
		- $\mathbb E(g(X))=\sum_{x\in\text{Im}(X)}g(x)P(X=x)$
	- důkaz
		- položme $Y=g(X)$
		- z definice: $\mathbb E(Y)=\sum_{y\in\text{Im}(Y)}y P(Y=y)$
		- zjevně $P(Y=y)=\sum_{x\in\text{Im}(X),g(x)=y} P(X=x)$
			- protože $y$ může být obrazem více různých $x$
		- dosadíme $\mathbb E(Y)=\sum_{y\in\text{Im}(Y)}y \sum_{x\in\text{Im}(X),g(x)=y} P(X=x)$
		- tedy $\mathbb E(Y)=\sum_{y\in\text{Im}(Y)} \sum_{x\in\text{Im}(X),g(x)=y}y P(X=x)$
		- $\mathbb E(Y)=\sum_{x\in\text{Im}(X)}g(x) P(X=x)$
	- linearita $\mathbb E(aX+b)=a\mathbb E(X)+b$ plyne z PNS pro funkci $ax+b$
	- podmíněná střední hodnota $\mathbb E(X\mid B)=\sum_{x\in\text{Im}(X)}x\cdot P(X=x\mid B)$
	- věta o celkové střední hodnotě: $\mathbb E(X)=\sum_i P(B_i)\cdot \mathbb E(X\mid B_i)$
- Alternativní vzorec střední hodnoty pomocí survival funkce
	- $\mathbb E(X)=\sum_{n=0}^\infty P(X\gt n)$
	- idea důkazu
		- $\mathbb E(X)=p_X(1)+p_X(2)+p_X(3)+\dots$
			- $+\,p_X(2)+p_X(3)+\dots$
			- $+\,p_X(3)+\dots$
- Rozptyl a jeho vlastnosti
	- rozptyl … $\text{var}(X)=\mathbb E((X-\mathbb EX)^2)$
	- směrodatná odchylka … $\sigma_X=\sqrt{\text{var}(X)}$
	- variační koeficient … $\text{CV}_X=\sigma_X/\mathbb E(X)$
	- věta: $\text{var}(X)=\mathbb E(X^2)-\mathbb E(X)^2$
	- důkaz: $\mathbb E((X-\mu)^2)=\mathbb E(X^2-2\mu X+\mu^2)=\mathbb E(X^2)-2\mu\mathbb E(X)+\mu^2$
- Náhodný vektor: sdružená pravděpodobnostní funkce a její vztah s funkcemi marginálními
	- $p_{X,Y}(x,y)=P(\set{\omega\in\Omega:X(\omega)=x\land Y(\omega)=y})$
	- $p_X(x)=\sum_{y} p_{X,Y}(x,y)$
		- podobně pro $p_Y(y)$
		- využíváme toho, že máme disjunktní sjednocení
- Pravděpodobnostní funkce libovolné funkce dvou náhodných veličin
	- věta
		- mějme náhodný vektor $(X,Y)$ a funkci $g:\mathbb R^2\to\mathbb R$
		- pak $Z=g(X,Y)$ je náhodná veličina na $(\Omega,\mathcal F,P)$
		- přičemž $p_Z(z)=\sum_{x\in\text{Im}(X),y\in\text{Im}(Y): g(x,y)=z} P(X=x\land Y=y)$
	- důkaz
		- $P((X,Y)\in A)=\sum_{a\in A} P((X,Y)=a)$
			- plyne přímo z pravděpodobnosti disjunktního sjednocení (je to součet pravděpodobností)
- Nezávislost náhodných veličin
	- $X,Y$ jsou nezávislé, jestliže pro každé $x,y\in\mathbb R$ jsou jevy $\set{X=x}$ a $\set{Y=y}$ nezávislé
	- to nastane, právě když $P(X=x,Y=y)=P(X=x)\cdot P(Y=y)$
	- neboli $p_{X,Y}(x,y)=p_X(x)\cdot p_Y(y)$
- PNS pro funkci náhodného vektoru, střední hodnota součtu n.v., součinu nezávislých n.v.
	- $\mathbb E(g(X,Y))=\sum_x\sum_y g(x,y) P(X=x\land Y=y)$
		- vyplývá ze vzorce pro pravděpodobnostní funkci funkce dvou náhodných veličin
	- $\mathbb E(aX+bY)=a\mathbb E(X)+b\mathbb E(Y)$
		- vyplývá z PNS
	- $\mathbb E(XY)=\mathbb E(X)\mathbb E(Y)$
		- vyplývá z PNS
- Konvoluční vzorec
	- pravděpodobnostní funkce součtu $Z=X+Y$
		- $P(Z=z)=\sum_{x\in\text{Im}(X)} P(X=x\land Y=z-x)$
	- pro nezávislé $X,Y$
		- $P(Z=z)=\sum_{x\in\text{Im}(X)} P(X=x)P(Y=z-x)$
	- obojí vyplývá ze vzorce pro pravděpodobnostní funkci funkce dvou náhodných veličin
- Kovariance a její vlastnosti
	- definice: $\text{cov}(X,Y)=\mathbb E((X-\mathbb EX)(Y-\mathbb EY))$
	- věta: $\text{cov}(X,Y)=\mathbb E(XY)-\mathbb E(X)\mathbb E(Y)$
		- je to přímočaré roznásobení definice
	- pro nezávislé $X,Y$ platí $\text{cov}(X,Y)=0$
	- zjevně $\text{cov}(X,X)=\text{var}(X)$
	- $\text{cov}(aX+bY,Z)=a\text{cov}(X,Z)+b\text{cov}(Y,Z)$
	- korelace
		- $\rho(X,Y)=\frac{\text{cov}(X,Y)}{\sigma_X\cdot\sigma_Y}$
- Rozptyl součtu náhodných veličin
	- věta
		- nechť $X=\sum_{i=1}^n X_i$
		- pak $\text{var}(X)=\sum_{i=1}^n\sum_{j=1}^n\text{cov}(X_i,X_j)$
	- důkaz
		- $\text{var}(X)=\text{cov}(X,X)=\text{cov}(\sum X_i,\sum X_i)$
		- použiju $\text{cov}(aX+bY,Z)=a\text{cov}(X,Z)+b\text{cov}(Y,Z)$, rozložím všechny členy
	- věta
		- pro jevy po dvou nezávislé
		- $\text{var}(X)=\sum_{i=1}^n\sum_{j=1}^n\text{cov}(X_i,X_j)=\sum_{i=1}^n\text{var}(X_i)$
	- důkaz druhé rovnosti
		- $\text{cov}(X_i,X_j)=0\iff i\neq j$

## Spojité náhodné veličiny

- Distribuční funkce, její vlastnosti
	- distribuční funkce náhodné veličiny $X$ je funkce $F_X:\mathbb R\to [0,1]$ definovaná předpisem $F_X(x):=P(X\leq x)$
	- zjevně $P(a\lt X\leq b)=F_X(b)-F_X(a)$
	- vlastnosti
		- $F_X$ je neklesající
		- $\lim_{x\to+\infty} F_X(x)=1$
		- $\lim_{x\to-\infty} F_X(x)=0$
		- $F_X$ je zprava spojitá
- Spojité náhodné veličiny a jejich popis pomocí hustoty
	- náhodná veličina $X$ se nazývá spojitá, pokud existuje nezáporná reálná funkce $f_X$ tak, že $F_X(x)=P(X\leq x)=\int_{-\infty}^x f_X(t)\text dt$
	- hustota $f_X$ … „limita histogramů“
	- zjevně $\int_{-\infty}^\infty f=1$
- Využití hustoty – výpočet pravděpodobnosti intervalu, každý bod má pravděpodobnost nula
	- $P(X=x)=0$
	- $P(a\leq X\leq b)=\int_a^b f_X(t)\text dt$
- Střední hodnota u spojitých veličin: definice, pravidlo naivního statistika, rozptyl, linearita
	- $\mathbb E(X)=\int_{-\infty}^{\infty} x f_X(x)\text dx$
	- $\mathbb E(g(X))=\int_{-\infty}^\infty g(x)f_X(x)\text dx$
	- opět platí linearita střední hodnoty
	- $\text{var}(X)=\mathbb E((X-\mu)^2)=\int_{-\infty}^\infty (x-\mu)^2 f_X(x)\text dx$
	- opět platí $\text{var}(X)=\mathbb E(X^2)-(\mathbb E(X))^2$
		- z linearity střední hodnoty
- Příklady spojitých rozdělení: uniformní a exponenciální
	- uniformní $U(a,b)$
		- $f_X(x)=\frac 1{b-a}$
		- $F_X(x)=\frac{x-a}{b-a}$
		- $\mathbb E(X)=\frac{a+b}2$
		- $\text{var}(X)=\frac{(b-a)^2}{12}$
	- exponenciální $\text{Exp}(\lambda)$
		- $f_X(x)=\lambda e^{-\lambda x}$ pro $x\geq 0$
		- $F_X(x)=1-e^{-\lambda x}$ pro $x\geq 0$
		- $\mathbb E(X)=1/\lambda$
		- $\text{var}(X)=1/\lambda^2$
- Normální rozdělení
	- standardní normální rozdělení
		- $N(0,1)$
		- $f_X(x)=\varphi(x)=\frac1{\sqrt{2\pi}} e^{-x^2/2}$
	- obecné normální rozdělení
		- $N(\mu,\sigma^2)$
		- $f_X(x)=\frac 1{\sigma\sqrt{2\pi}}e^{-\frac12(\frac{x-\mu}{\sigma})^2}$
	- máme-li $X\sim N(\mu,\sigma^2)$, pak pro $Z=\frac{X-\mu}\sigma$ platí $Z\sim N(0,1)$
	- pro normální nezávislé náhodné veličiny $X_i\sim N(\mu_i,\sigma_i^2)$ (nechť je jich konečně) je i součet normální náhodná veličina $\sum X_i\sim N(\sum\mu_i,\sum\sigma_i^2)$
	- pravidlo $3\sigma$
		- $P(\mu-\sigma\lt X\lt\mu+\sigma)\doteq 0.68$
		- $P(\mu-2\sigma\lt X\lt\mu+2\sigma)\doteq 0.95$
		- $P(\mu-3\sigma\lt X\lt\mu+3\sigma)\doteq 0.997$
- Cauchyho rozdělení (jako varování)
	- $f(x)=\frac1{\pi(1+x^2)}$
	- nemá střední hodnotu
- Paretovo rozdělení
	- $F_X(x)=1-(\frac{x_0}x)^\alpha$ pro $x\geq x_0$ (jinak 0)
	- $f_X=\alpha x_0^\alpha/x^{\alpha+1}$ pro $x\geq x_0$ (jinak 0)
	- $\mathbb E(X)=x_0\frac\alpha{\alpha-1}$ pro $\alpha\gt 1$
- Kvantilová funkce stručně
	- $Q_x(p)=\min\set{x\in\mathbb R:p\leq F_X(x)}$
	- pro spojité $X$ platí $Q_X=F^{-1}_X$
	- medián … $Q_X(1/2)$
	- 1\. kvartil … $Q_X(1/4)$
	- 3\. kvartil … $Q_X(3/4)$
- Sdružená distribuční funkce
	- $F_{X,Y}(x,y)=P(\set{\omega\in\Omega:X(\omega)\leq x\land Y(\omega)\leq y})$
	- někdy lze použít $F_{X,Y}(x,y)=\int_{-\infty}^x\int_{-\infty}^y f_{X,Y}(x,y)\text dx\text dy$
		- pak $f_{X,Y}$ je sdružená hustota
- Pravděpodobnost obdélníku pomocí sdružené distribuční funkce
	- $P(X\in(a,b]\land Y\in(c,d])=F(b,d)-F(a,d)-F(b,c)+F(a,c)$
- Marginální hustota
	- $f_X(x)=\int_{y\in\mathbb R}f_{X,Y}(x,y)\text dy$
	- podobně $f_Y$
- Nezávislost (pomocí distribuční funkce i pomocí hustoty)
	- $F_{X,Y}(x,y)=F_X(x)F_Y(y)$
	- $f_{X,Y}(x,y)=f_X(x)f_Y(y)$
- Konvoluce pro spojité náhodné veličiny
	- pro nezávislé náhodné veličiny
	- $f_Z(z)=\int_{-\infty}^\infty f_X(x)f_Y(z-x)\text dx$
- Markovova nerovnost
	- pro $X\geq 0$ a $a\gt0$ platí $P(X\geq a)\leq\frac{\mathbb E(X)}a$
	- důkaz
		- $\mathbb E(X)=P(X\geq a)\cdot\mathbb E(X\mid X\geq a)+P(X\lt a)\cdot\mathbb E(X\mid X\lt a)$
		- $\geq P(X\geq a)\cdot a+0$
- Čebyševova nerovnost
	- $P(|X-\mu|\geq t\cdot\sigma)\leq\frac1{t^2}$
	- důkaz
		- položíme $Y=(X-\mu)^2$
			- zjevně $\mathbb E(Y)=\sigma^2$
		- použijeme Markovovu nerovnost pro $Y$
			- $P(Y\geq t^2\sigma^2)\leq\frac{\sigma^2}{t^2\sigma^2}$
- Silný zákon velkých čísel
	- mějme $X_1,X_2,\dots$ stejně rozdělené nezávislé náhodné veličiny
	- $\bar X_n=(X_1+\dots+X_n)/n$ … výběrový průměr
	- $\lim_{n\to\infty}\bar X_n=\mu$ skoro jistě (s pravděpodobností 1)
	- použití: Monte Carlo integrování kruhu
- Slabý zákon velkých čísel (zlepšení přesnosti opakovaným měřením)
	- věta
		- nechť $X_1,X_2,\dots$ jsou stejně rozdělené nezávislé náhodné veličiny
		- $\bar X_n=(X_1+\dots+X_n)/n$ … výběrový průměr
		- $\forall\epsilon\gt 0:\lim_{n\to\infty} P(|\bar X_n-\mu|\gt\varepsilon)=0$
		- říkáme, že $\bar X_n$ konverguje k $\mu$ v pravděpodobnosti, píšeme $\bar X_n\xrightarrow P\mu$
	- důkaz
		- $\mathbb E\bar X_n=\mathbb E\frac{X_1+X_2+\dots+X_n}{n}=\frac{\mu n}n=\mu$
		- $\text{var}\bar X_n=\text{var}\frac{X_1+\dots+X_n}{n^2}=\frac{n\cdot\sigma^2}{n^2}=\frac{\sigma^2}n$
		- použijeme Čebyševovu nerovnost pro $t=\sqrt n\varepsilon/\sigma$, přičemž za $\sigma$ musíme dosadit $\sigma/\sqrt{n}$
		- $P(|\bar X_n-\mu|\geq \varepsilon)\leq\frac{\sigma^2}{n\cdot\varepsilon^2}$
- Centrální limitní věta – znění, vysvětlení
	- nechť $X_1,X_2,\dots$ jsou stejně rozdělené se střední hodnotou $\mu$ a rozptylem $\sigma^2$
	- označme $Y_n=\frac{(X_1+\dots+X_n)-n\mu}{\sigma\sqrt{n}}$
	- pak $Y_n\xrightarrow d N(0,1)$
		- $Y_n$ konverguje v distribuci k $N(0,1)$
		- tzn. $\lim_{n\to\infty} F_{Y_n}(x)=\Phi(x)$
	- CLV se hodí k aproximaci distribuce součtu nebo průměru velkého počtu náhodných veličin normálním rozdělením
		- takže můžeme provádět bodové a intervalové odhady i tam, kde data nejsou normálně rozdělená, ale známe rozptyl

## Statistika

- Explorační vs. konfirmační analýza
	- explorační analýza – něco počítáme pro napozorovaná data, objevujeme zajímavé zákonitosti
	- konfirmační analýza – ověřujeme, jestli jsou zákonitosti pravdivé
- Odhady konzistentní a (asymptoticky) nevychýlené, vychýlení (bias) a střední kvadratická chyba
	- pro náhodný výběr $X_1,\dots,X_n\sim F_\theta$ a libovolnou funkci $g$ nazveme bodový odhad $\hat\theta_n$
		- nevychýlený/nestranný, pokud $\mathbb E(\hat\theta_n)=g(\theta)$
		- asymptoticky nevychýlený, pokud $\lim_{n\to\infty}\mathbb E(\hat\theta_n)=g(\theta)$
		- konzistentní, pokud $\hat\theta_n\xrightarrow P g(\theta)$
	- dále definujeme
		- vychýlení … $\text{bias}(\hat\theta_n)=\mathbb E(\hat\theta_n)-\theta$
		- střední kvadratickou chybu … $\text{MSE}(\hat\theta_n)=\mathbb E((\hat\theta_n-\theta)^2)$
	- věta: $\text{MSE}(\hat\theta_n)=\text{bias}(\hat\theta_n)^2+\text{var}(\hat\theta_n)$
	- důkaz
		- $\text{var}(\hat\theta_n)=\text{var}(\hat\theta_n-\theta)$
			- posunutím se rozptyl nezmění
		- $=\mathbb E((\hat\theta_n-\theta)^2)-(\mathbb E(\hat\theta_n-\theta))^2$
			- podle věty o výpočtu rozptylu
		- první člen je MSE, druhý je druhá mocnina biasu (pak už stačí jen upravit rovnici)
- Konstrukce odhadů pomocí metody momentů i maximální věrohodnosti
	- metoda momentů
		- $r$-tý moment $X$ … $\mathbb EX^r=m_r(\theta)$
		- $r$-tý výběrový moment … $\frac1n\sum_{i=1}^n X_i^r=\widehat{m_r}$
			- konzistentní nevychýlený odhad pro $r$-tý moment
		- nalezneme $\theta$ takové, že $m_r(\theta)=\widehat{m_r}$
		- typicky stačí použít první moment, dostaneme nějakou rovnici
		- $m_1=\mu$
		- $m_2=\mathbb E(X^2)=\text{var}(X)+(\mathbb EX)^2=\sigma^2+\mu^2$
	- metoda maximální věrohodnosti
		- $\hat\theta_{ML}=\text{argmax}_\theta\;p(x;\theta)$
			- $\text{argmax}_\theta\;f(x;\theta)$
			- abych se nemusel rozhodovat mezi $p$ a $f$, budu používat $L$
		- výpočetně jednodušší bude používat logaritmus $L$, který označíme $\ell$
		- příklad
			- ve vzorku $k$ leváků z $n$ lidí, hledáme pravděpodobnost $\theta$, že je někdo levák
			- $L(x;\theta)=\theta^k(1-\theta)^{n-k}$
			- $\ell(x;\theta)=k\log\theta+(n-k)\log(1-\theta)$
			- $\ell'(x;\theta)=\frac k\theta-\frac{n-k}{1-\theta}$
				- hledáme maximum, položíme derivaci rovnou nule (a zkontrolujeme krajní hodnoty)
		- podobně pro spojitý případ – „maximalizujeme“ rovnici pro pravděpodobnost konkrétního výběru
- Výběrový průměr a rozptyl
	- populační vs. výběrový průměr/rozptyl
		- populační … pro celou populaci
		- výběrový … pro konkrétní vzorek dat
	- $\overline{X_n}=\frac1n\sum_{i=1}^n X_i$
		- konzistentní nevychýlený odhad $\mu$
	- $\widehat{S_n^2}=\frac1{n-1}\sum_{i=1}^n(X_i-\overline{X_n})^2$
		- konzistentní nevychýlený odhad $\sigma^2$
	- proč se rozptyl dělí $n-1$
		- $\mathbb EX_i=\mu\implies\mathbb E\overline{X_n}=\mu$
		- $\text{var}(X_i)=\sigma^2\implies\text{var}(\overline{X_n})=\frac{\sigma^2}n$
		- $\sigma(X_i)=\sigma\implies\sigma(\overline{X_n})=\frac\sigma{\sqrt{n}}$
		- více viz záznam přednášky
- Intervalové odhady – obecná metoda založená na normálním rozdělení
	- statistiky $D\leq H$ určují konfidenční interval o spolehlivosti $1-\alpha$, pokud $P(D\leq\theta\leq H)=1-\alpha$
		- zkráceně $(1-\alpha)$-CI
	- interval budeme uvažovat ve tvaru $[x-\delta,x+\delta]$
	- postup
		- máme nestranný bodový odhad $\hat\theta$ pro parametr $\theta$
		- $\hat\theta$ má normální rozdělení
		- $\hat\theta\pm z_{\alpha/2}\cdot\text{se}$ je $(1-\alpha)$-CI
			- $z_{\alpha/2}:=\Phi^{-1}(1-\alpha/2)$
			- $\text{se}:=\sigma(\hat\theta)$
	- idea
		- provedeme standardizaci $Z=\text{stand}(\hat\theta)=\frac{\hat\theta-\mathbb E(\hat\theta)}{\sigma(\hat\theta)}=\frac{\hat\theta-\theta}{\sigma(\hat\theta)}$
			- tohle má normální rozdělení
- Schéma testování hypotéz: nulová hypotéza, alternativní hypotéza, hladina významnosti
	- nulová hypotéza … defaultní, konzervativní model
	- alternativní hypotéza … alternativní model, „zajímavost“
	- nulovou hypotézu buď zamítneme, nebo nezamítneme
	- chyba 1. druhu – chybné zamítnutí, „trapas“
	- chyba 2. druhu – chybné přijetí, „promarněná příležitost“
	- hladina významnosti $\alpha$ … pravděpodobnost chyby 1. druhu
		- typicky se volí $\alpha=0.05$
	- $\beta$ … pravděpodobnost chyby 2. druhu
	- kritický obor … je to množina, kterou určíme před provedením testu; pokud se výsledek našeho testu bude nacházet v kritickém oboru, zamítneme nulovou hypotézu
		- tedy $\alpha=P(h(X)\in W; H_0)$
	- síla testu … $1-\beta$
		- chceme co největší
	- $p$-hodnota … nejmenší $\alpha$ taková, že na hladině $\alpha$ zamítáme $H_0$
- Testování střední hodnoty normálního rozdělení (známý vs. neznámý rozptyl, neboli z-test vs. t-test)
	- známe rozptyl
		- teorie: $Z=\frac{\overline{X_n}-\theta_0}{\sigma/\sqrt n}\sim N(0,1)$ pokud $H_0$
			- podle centrální limitní věty tohle funguje i pro veličiny, které nemají normální rozdělení, ale známe jejich rozptyl
		- najdeme bodový odhad $\hat\theta$ pro $\theta$
			- třeba pokud nás zajímá $\mu$, tak prostě použijeme výběrový průměr
		- $\delta=\frac{\sigma}{\sqrt{n}}\cdot \Phi^{-1}(1-\alpha/2)$
		- vrátíme $[\hat\theta-\delta,\hat\theta+\delta]$
	- neznáme rozptyl
		- teorie: $T$ bude jako $Z$, akorát místo $\sigma$ použijeme $\bar\sigma$
		- máme $n$ hodnot
		- spočteme výběrovou odchylku $\bar\sigma$
		- $\delta=\frac{\bar\sigma}{\sqrt n}\cdot \Psi^{-1}_{n-1}(1-\alpha/2)$
		- zbytek stejný, jako když známe rozptyl
- Numerická/ordinální/kategorická data
	- numerická data … důležitá jsou čísla
	- ordinální data … hodnoty mají pořadí (třeba úrovně vzdělání), ale nemají žádný číselný význam
	- kategorická data … hodnoty nemají žádné konkrétní pořadí, jde o kategorie
- Multinomické rozdělení
	- $n$-krát opakuji pokus, kde může nastat jedna z $k$ možností, přičemž $i$-tá má pravděpodobnost $p_i$
	- $X_i$ … kolikrát nastala $i$-tá možnost
	- pak $(X_1,\dots,X_k)$ má multinomické rozdělení s parametry $n,(p_1,\dots,p_k)$
	- příklad: hážeme 6stěnnou kostkou
- Test dobré shody (G-test, $χ^2$-test) – předvedení a částečné zdůvodnění
	- $O_i$ … reálný výsledek
	- $E_i$ … očekávaný výsledek
	- proč uvažujeme $n-1$ stupňů volnosti, když máme $n$ hodnot?
		- z $n-1$ hodnot můžu $n$-tou hodnotu dopočítat
	- $\chi^2=\sum_i\frac{(E_i-O_i)^2}{E_i}$
	- $G=2\sum_i O_i \ln\frac{O_i}{E_i}$
	- $\chi^2$ a $G$ se přibližně rovnají (díky aproximaci pomocí Taylorova polynomu)
	- jak zjistit, jestli je kostka spravedlivá?
		- spočítáme $\chi^2$ na základě měření
		- najdeme v tabulce $p$-hodnotu pro konkrétní hodnotu $\chi^2$ s 5 stupni volnosti
		- hypotézu „kostka je spravedlivá“ zamítneme pro dostatečně nízkou $p$-hodnotu
- Jednovýběrový vs. dvouvýběrový test vs. párový test
	- jednovýběrový test … klasický intervalový odhad
		- např. $H_0$ … $\mu=5$
	- dvouvýběrový test … máme dvě sady dat, porovnáváme parametry
		- např. $H_0$ … $\mu_X=\mu_Y$
	- párový test … data jsou ve dvojicích
		- např. $H_0$ … $\mu_X=\mu_Y$
		- dvojice dat spolu nějak souvisí
		- uvažuju $R_i=Y_i-X_i$, použiju jednovýběrový test
	- příklad použití dvouvýběrového testu
		- „jsou stejné střední hodnoty?“
		- hledáme $\theta=\mu_X-\mu_Y$
		- známe-li $\sigma_X,\sigma_Y$, pak $\sigma(\hat\theta)=\sqrt{\frac{\sigma_X^2}n+\frac{\sigma_Y^2}m}$
- Lineární regrese (a možné komplikace)
	- $x_i$ … nezávislá proměnná, predictor
	- $y_i$ … závislá proměnná, response
	- cíl … $y=\theta_0+\theta_1x$
		- $\theta_0$ … intercept
		- $\theta_1$ … slope
	- chybu měříme pomocí kvadratické odchylky $\sum_{i=1}^n(y_i-(\theta_0+\theta_1x_i))^2$
	- řešení
		- $\hat\theta_1=\frac{cov(x,y)}{var(x)}$
			- použijeme výběrový rozptyl a výběrovou kovarianci
		- $\hat\theta_0=\bar y-\theta_1\bar x$
	- komplikace
		- někdy nechceme provádat lineární regresi – je fajn se podívat na graf
		- zavádějící proměnná (confounding variable) – není v datech, ale kdybychom ji přidali, všechno by dávalo větší smysl
		- Simpsonův paradox – jedna strana vítězí v jednotlivých kategoriích, ale dohromady vítězí ta druhá
- Neparametrické testy – vlastnosti empirické distribuční funkce (KS test)
	- když nemůžu distribuci popsat pomocí parametrů nějaké obvyklé distribuční funkce → neparametrická statistika
	- empirická distribuční funkce … $\hat F_n(x)=$ počet dobrých / počet všech
		- $\hat F_n(x)=\frac{\sum_{i=1}^n I(X_i\leq x)}{n}$
		- $\hat F_n(x)=$ jaký poměr hodnot je nejvýš $x$
	- vlastnosti
		- střední hodnota $\hat F_n(x)$ je $F(x)$
		- $\hat F_n(x)\xrightarrow P F(x)$
			- podle slabého zákona velkých čísel
		- KS test (věta)
			- pravděpodobnost, že $F(x)$ leží v pásku $\hat F_n(x)\pm\varepsilon$ je aspoň $1-\alpha$
			- přičemž $\varepsilon=\sqrt{\frac1{2n}\log\frac2\alpha}$
- Generování náhodných veličin (inverzní transformace, rejection sampling)
	- uniformní rozdělení $U(0,1)$ – dejme tomu, že ho máme (je těžké ho generovat)
	- diskrétní náhodná veličina – uděláme rozklad intervalu od nuly do jedné tak, aby $P(X=i)=|A_i|$, kde $|A_i|$ je část intervalu
	- inverzní transformace
		- $Q_X(p)=F_X^{-1}(p)$ pro $X$ spojitou
		- $Q_X(p)=\min\set{x:F_X(x)\geq p}$
		- zjevně $Q(p)\leq x\iff p\leq F(x)$
		- věta: $Q(U)$ má distribuční funkci $F$
			- kde $U\sim U(0,1)$
		- důkaz
			- mějme $X=Q(U)$
			- $P(X\leq x)=P(Q(U)\leq x)=P(U\leq F(x))=F(x)$
	- rejection sampling
		- generujeme uniformně náhodně bod $(x,y)$ pod křivkou $f_X$
		- pak $x$ má hustotu $f_X$
		- generovat bod pod křivkou je těžké → generujeme body všude a zahodíme ty, které nejsou pod křivkou
